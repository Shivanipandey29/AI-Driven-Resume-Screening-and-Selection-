{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15337b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "\n",
    "job_description = \"\"\"\n",
    "Artefact is a new generation of data service providers specialising in data consulting and data-driven digital marketing. It is dedicated to transforming data into business impact across the entire value chain of organisations. We are proud to say we’re enjoying skyrocketing growth.\n",
    "\n",
    "\n",
    "The backbone of our consulting missions, today our Data consulting team has more than 400 consultants covering all Artefact's offers (and more): data marketing, data governance, strategy consulting, product owner…\n",
    "\n",
    "\n",
    "What you will be doing?\n",
    "\n",
    "As a Data Scientist, your primary role revolves around leveraging Python and SQL to conduct data science modelling, employing statistical and machine learning algorithms to derive actionable insights. Your responsibilities will encompass:\n",
    "Data Modeling: Developing and implementing sophisticated data science models to extract valuable insights from complex datasets.\n",
    "Algorithm Implementation: Applying statistical and machine learning algorithms to solve business problems and enhance decision-making processes.\n",
    "Visualization: Utilizing visualization tools like Tableau and PowerBI to present findings in a compelling and understandable manner.\n",
    "Cloud-Based Deployment: Deploying models on cloud platforms such as MS Azure, GCP, and AWS, optimizing their functionality and scalability.\n",
    "\n",
    "\n",
    "What we are looking for?\n",
    "\n",
    "Experience Levels:\n",
    "\n",
    "0-2 years: Associate Data Scientist\n",
    "2-3 years: Data Scientist\n",
    "3-5 years: Senior Data Scientist\n",
    "5-8 years: Lead Data Scientist\n",
    "Technical Expertise:\n",
    "\n",
    "Proficiency in Python, SQL, data science modeling, and statistical analysis.\n",
    "Strong grasp of ML algorithms and their implementation in real-world scenarios.\n",
    "Experience in visualization tools like Tableau and PowerBI, along with knowledge of deploying models on cloud platforms (MS Azure, GCP, AWS).\n",
    "Attitude & Soft Skills:\n",
    "\n",
    "Proven problem-solving skills and a solution-oriented mindset.\n",
    "Excellent communication skills to collaborate effectively within teams and with stakeholders.\n",
    "Strong business acumen with an interest in business-facing roles.\n",
    "Adaptability and a start-up mentality to thrive in a dynamic environment.\n",
    "Where do you work?\n",
    "\n",
    "Candidates with similar skill sets and experiences have excelled in technology firms or consultancy firms.\n",
    "Academic Background:\n",
    "\n",
    "Successful candidates often possess Computer Science, Electronics and Communication Engineering degrees.\n",
    "\n",
    "\n",
    "Artefact is the place to be: come and build the future of data and AI\n",
    "Innovation: We have a passion for creating impacting projects and believe innovation can come from anyone.\n",
    "Action: We make things rather than telling people how to make them.\n",
    "Collaboration: We believe in bringing talented people together, winning together, and in learning from each other.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Access the stopwords using\n",
    "\n",
    "# Assuming find_all_text is a function that returns a list of strings\n",
    "resume_paths = [ \n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\shivani_pandey (1).pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\Adityaresume - Prince Adi.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\afnancv - afnan shaikh.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\amaan_resume - Shaikh Amaan.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\Ankita Pawar Resume - Ankita Pawar.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\Karan_Resume - Karan G (1).pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\My__Resume - Priyanka Wankhede.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\NIKITA (2) - Nikita Shewale.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\resume - shankar dhadake (1).pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\resume (1) - Shaikh Aryan.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\resume (2) - Nikita Singar.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\Resume_Rahul_Mashalkar - Rahul Mashalkar (1).pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\Resume2 - Shiwanjali Jadhav (1).pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\vaishali resume - Vaishali Shinde (1).pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\# Mahesh Gaikwad Resume - Mahesh Gaikwad.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\# Pratiksha_Chikhale_Resume - Pratiksha Chikhale.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\data sci resume - Sakshi More.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\NikitaS - Nikita Shewale.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\NikitaShinde_Overleaf - Nikita Shinde.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\Resume - Ajay Meherkar (1).pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\resume - Dipali Walunj (1).pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\resume - shankar dhadake (1).pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\resume (1) - Omkar Dhore.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\resume (2) - Rutika Pawar.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\Resume (3) - Vaishnavi Sonar.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\resume (4) - Nikita Uphade.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\Resume_Final 01-01-2024 - kajal laygude.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\resume-1 - Revati More (1).pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\Rohini Jadhav (Resume) - Rohini Jadhav (2).pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\Vinayak_Sakpal_Resume - Vinayak Sakpal.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\Vishakha_Pasalkar_Resume - Vishakha Pasalkar.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\Resume_Project (1).pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\new resumess\\rushikesh - Rushikesh Shigwan.pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\Umesh Chaudhari - Umesh Chaudhari (3).pdf\",\n",
    "                r\"C:\\Users\\write\\OneDrive\\Desktop\\new_file\\Pooja Jogdand (2).pdf\"\n",
    " ]         \n",
    "               \n",
    "\n",
    "def find_all_text(resume_paths):\n",
    "    # Load the spaCy English model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def extract_info_from_resume(pdf_path):\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "            text = ''\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                text += pdf_reader.pages[page_num].extract_text()\n",
    "\n",
    "        # Process the text with spaCy\n",
    "        doc = nlp(text)\n",
    "\n",
    "\n",
    "        # Extract Education\n",
    "        education = extract_education(doc)\n",
    "\n",
    "        # Extract Skills\n",
    "        skills = extract_skills(text)\n",
    "\n",
    "        # Extract Projects\n",
    "        projects = extract_projects(doc)\n",
    "\n",
    "        # Extract Experience using specific keywords\n",
    "        experience = extract_experience_sentences(doc)\n",
    "\n",
    "        # Concatenate information into the 'All_Text' column\n",
    "        all_text = f\"Education: {education}\\n Skills: {skills}\\n Projects: {projects}\\n Experience: {experience}\"\n",
    "\n",
    "        #print(f\"All_Text: {all_text}\")\n",
    "\n",
    "        return {\n",
    "            'Education': education,\n",
    "            'Skills': skills,\n",
    "            'Projects': projects,\n",
    "            'Experience': experience,\n",
    "            'All_Text': all_text\n",
    "        }\n",
    "\n",
    "\n",
    "    def extract_education(doc):\n",
    "        # Initialize an empty list to store education information\n",
    "        education_info = []\n",
    "\n",
    "        # Define education-related keywords and labels\n",
    "        education_keywords = ['education', 'university', 'college', 'Masters', 'Bachelors', 'degree', 'institute', \n",
    "                              'institutes', 'school', 'graduation', 'postgraduate', 'undergraduate', 'diploma', \n",
    "                              'academy', 'IIT', 'BIT', 'NIT', 'VIT', 'VIIT', 'COEP', 'SIT', 'MIT', 'SIBM',\n",
    "                              'AISSMS', 'PICT', 'PCCOE', 'SPPU', 'DY PATIL', 'DIAT', 'AIT', 'MMCOE', 'SLS',\n",
    "                              'SYMBIOSIS', 'FC', 'AFMS', 'MILIT', 'SINHGAD', 'MCASC', 'SID', 'BMCC', 'SCAC', \n",
    "                              'IMED', 'SIIB', 'SIDTM', 'IIMS', 'FLAME', 'PUMBA', 'DYPIMS', 'NIA', 'NIBM', 'SSE', \n",
    "                              'IIIT', 'SCMIRT', 'IBMR', '%']\n",
    "\n",
    "        education_labels = ['EDU', 'ORG', 'PRODUCT', 'UNI', 'INST', 'SCH', 'DEG', 'DIP', 'CERT', 'ACAD', 'UNIV', 'COLL', 'SCHOOL']\n",
    "\n",
    "        # Iterate through entities to find education-related information\n",
    "        for ent in doc.ents:\n",
    "            # Check if the entity is related to education\n",
    "            if any(keyword in ent.text.lower() for keyword in education_keywords) and ent.label_ in education_labels:\n",
    "                # Extract the surrounding text to get the full education information\n",
    "                start = max(0, ent.start - 1)  # Adjust the window size as needed\n",
    "                end = min(len(doc), ent.end + 25)\n",
    "                education_info.append(doc[start:end].text.strip())\n",
    "\n",
    "        return ', '.join(education_info) if education_info else 'N/A'\n",
    "\n",
    "\n",
    "\n",
    "    def extract_skills(text):\n",
    "        skills_start = text.find('Skills')\n",
    "        skills_end = text.find('SKILLS') if text.find('SKILLS') != -1 else len(text)\n",
    "        skills_text = text[skills_start:skills_end].strip()\n",
    "\n",
    "        # Process the skills text with spaCy\n",
    "        doc = nlp(skills_text)\n",
    "\n",
    "        # Extract lines or sentences containing skill-related information\n",
    "        skill_sentences = [sent.text.strip() for sent in doc.sents if any(token.pos_ in ['NOUN', 'VERB', 'ADJ'] for token in sent)]\n",
    "\n",
    "        # Combine the extracted lines into a single string\n",
    "        skills = ' '.join(skill_sentences)\n",
    "\n",
    "        return skills if skills else 'N/A'\n",
    "\n",
    "    def extract_projects(doc):\n",
    "        # Initialize an empty list to store project information\n",
    "        projects = []\n",
    "\n",
    "        # Define project-related keywords and labels\n",
    "        project_keywords = ['project', 'developed', 'created', 'implemented', 'built']\n",
    "        project_labels = ['PROJECT', 'ORG', 'PRODUCT']\n",
    "\n",
    "        # Iterate through sentences to find project-related information\n",
    "        for ent in doc.ents:\n",
    "            # Check if the entity is related to projects\n",
    "            if any(keyword in ent.text.lower() for keyword in project_keywords) and ent.label_ in project_labels:\n",
    "                # Extract the surrounding text to get the full project information\n",
    "                start = max(0, ent.start - 1)  # Adjust the window size as needed\n",
    "                end = min(len(doc), ent.end + 135)\n",
    "                project_info = doc[start:end].text.strip()\n",
    "                projects.append(project_info)\n",
    "\n",
    "        return ', '.join(projects) if projects else 'N/A'\n",
    "\n",
    "\n",
    "    def extract_experience_sentences(doc):\n",
    "        # Find the start and end indices of the \"Experience\" section\n",
    "        start_index = doc.text.lower().find('experience')\n",
    "        end_index = doc.text.lower().find('skills') if doc.text.lower().find('skills') != -1 else len(doc.text)\n",
    "\n",
    "        # Extract sentences within the specified section\n",
    "        experience_section = doc.text[start_index:end_index]\n",
    "\n",
    "        # Process the experience section with spaCy\n",
    "        experience_doc = nlp(experience_section)\n",
    "\n",
    "        # Extract sentences containing keywords related to experience within the section\n",
    "        experience_keywords = ['work', 'experience', 'position', 'job']\n",
    "        experience_sentences = [sent.text.strip() for sent in experience_doc.sents if any(keyword in sent.text.lower() for keyword in experience_keywords)]\n",
    "\n",
    "        # Combine the extracted sentences into a single string\n",
    "        experience = ' '.join(experience_sentences)\n",
    "\n",
    "        return experience if experience else 'N/A'\n",
    "\n",
    "\n",
    "    def clean_and_tokenize_skills(skill_string):\n",
    "        if isinstance(skill_string, str):\n",
    "            # Remove '/', '\\n', extra spaces\n",
    "            cleaned_string = re.sub(r'[/\\\\]+|\\n|\\s+', ' ', skill_string)\n",
    "\n",
    "            # Tokenize the skills using nltk's word_tokenize\n",
    "            tokenized_skills = word_tokenize(cleaned_string)\n",
    "\n",
    "            return tokenized_skills\n",
    "        else:\n",
    "            # Return an empty list if the input is not a string\n",
    "            return []\n",
    "\n",
    "\n",
    "    all_resume_info = []\n",
    "\n",
    "    for resume_path in resume_paths:\n",
    "        resume_info = extract_info_from_resume(resume_path)\n",
    "        all_resume_info.append(resume_info)\n",
    "    all_text=[]\n",
    "    #df_candidates = pd.DataFrame(all_resume_info, columns=['Name', 'Email', 'Education', 'Skills', 'Projects', 'Experience','All_Text'])\n",
    "    for k in range(0,len(all_resume_info)):\n",
    "        for i,j in all_resume_info[k].items():\n",
    "            if i==\"All_Text\":\n",
    "                all_text.append(j)\n",
    "            \n",
    "    return all_text\n",
    "\n",
    "def remove_un(text):\n",
    "    if type(text) == str:\n",
    "        string = []\n",
    "        for i in text.split():\n",
    "            word = (\"\".join(e for e in i if e.isalnum()))\n",
    "            word = word.lower()\n",
    "\n",
    "            if word not in stopwords:\n",
    "                string.append(word)\n",
    "\n",
    "        return \" \".join(string)\n",
    "    elif type(text) == list:\n",
    "        result = []\n",
    "        for t in text:\n",
    "            if type(t) == str:\n",
    "                string = []\n",
    "                for i in t.split():\n",
    "                    word = (\"\".join(e for e in i if e.isalnum()))\n",
    "                    word = word.lower()\n",
    "\n",
    "                    if word not in stopwords:\n",
    "                        string.append(word)\n",
    "                result.append(\" \".join(string))\n",
    "\n",
    "        return result\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_name_from_pdf(pdf_path):\n",
    "    # Extract the base name (file name) from the file path\n",
    "    file_name = os.path.basename(pdf_path)\n",
    "    \n",
    "    # Remove the file extension (.pdf)\n",
    "    name_without_extension = os.path.splitext(file_name)[0]\n",
    "    \n",
    "    # Split the name using underscores or spaces\n",
    "    name_parts = re.split(r'[_\\s]+', name_without_extension)\n",
    "    \n",
    "    # Capitalize each part of the name\n",
    "    capitalized_name = ' '.join(part.capitalize() for part in name_parts)\n",
    "    \n",
    "    return capitalized_name    \n",
    "\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "# Function to convert text to vector\n",
    "def text_to_vector(text):\n",
    "    words = preprocess_text(text)\n",
    "    # Get word vectors for each word\n",
    "    word_vectors = [token.vector for token in nlp(\" \".join(words))]\n",
    "    # Average the word vectors to get the document vector\n",
    "    if word_vectors:\n",
    "        return sum(word_vectors) / len(word_vectors)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to convert job description to vector\n",
    "def job_to_vector(job_description):\n",
    "    return text_to_vector(job_description)\n",
    "\n",
    "# Function to calculate cosine similarity between two vectors\n",
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    if vector1 is not None and vector2 is not None:\n",
    "        return cosine_similarity([vector1], [vector2])[0][0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "resume = find_all_text(resume_paths)\n",
    "\n",
    "resume_final = []\n",
    "for r in resume:\n",
    "    cleaned_resume = remove_un(r)\n",
    "    resume_final.append(cleaned_resume)\n",
    "\n",
    "job_description = remove_un(job_description)\n",
    "\n",
    "\n",
    "name=[]\n",
    "for pdf_path in resume_paths:\n",
    "    person_name = extract_name_from_pdf(pdf_path)\n",
    "    name.append(person_name)\n",
    "    \n",
    "\n",
    "# Example student profiles\n",
    "student_profiles = resume_final\n",
    "\n",
    "# Example job description\n",
    "job_description = job_description\n",
    "\n",
    "# Convert each student profile to a vector\n",
    "profile_vectors = [text_to_vector(profile) for profile in student_profiles]\n",
    "\n",
    "# Convert job description to a vector\n",
    "job_vector = job_to_vector(job_description)\n",
    "\n",
    "# Calculate cosine similarity between each profile and the job description\n",
    "similarities = [calculate_cosine_similarity(profile_vector, job_vector) for profile_vector in profile_vectors]\n",
    "\n",
    "    \n",
    "# Zip the names with the similarities\n",
    "name_similarity_pairs = list(zip(name, similarities))\n",
    "\n",
    "# Sort the pairs based on similarity in descending order\n",
    "sorted_name_similarity_pairs = sorted(name_similarity_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted similarities with names\n",
    "for i, (person_name, similarity) in enumerate(sorted_name_similarity_pairs):\n",
    "    print(f\"Similarity between {person_name} and Job Description: {similarity}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def extract_skills_from_resume(text):\n",
    "    # Extract Skills\n",
    "    skills = extract_skills(text)\n",
    "    return skills\n",
    "\n",
    "def extract_skills(text):\n",
    "    skills_start = text.find('Skills')\n",
    "    skills_end = text.find('SKILLS') if text.find('SKILLS') != -1 else len(text)\n",
    "    skills_text = text[skills_start:skills_end].strip()\n",
    "\n",
    "    # Process the skills text with spaCy\n",
    "    doc = nlp(skills_text)\n",
    "\n",
    "    # Extract lines or sentences containing skill-related information\n",
    "    skill_sentences = [sent.text.strip() for sent in doc.sents if any(token.pos_ in ['NOUN', 'VERB', 'ADJ'] for token in sent)]\n",
    "\n",
    "    # Combine the extracted lines into a single string\n",
    "    skills = ' '.join(skill_sentences)\n",
    "\n",
    "    return skills if skills else 'N/A'\n",
    "\n",
    "def clean_and_tokenize_skills(skill_string):\n",
    "    if isinstance(skill_string, str):\n",
    "        # Remove '/', '\\n', extra spaces\n",
    "        cleaned_string = re.sub(r'[/\\\\]+|\\n|\\s+', ' ', skill_string)\n",
    "\n",
    "        # Tokenize the skills using nltk's word_tokenize\n",
    "        tokenized_skills = word_tokenize(cleaned_string)\n",
    "\n",
    "        return tokenized_skills\n",
    "    else:\n",
    "        # Return an empty list if the input is not a string\n",
    "        return []\n",
    "\n",
    "def fetch_my_score(resume_text):\n",
    "    all_skills = extract_skills_from_resume(resume_text)\n",
    "    all_skills = \"\".join(all_skills)\n",
    "    cleaned_skills = clean_and_tokenize_skills(all_skills)\n",
    "\n",
    "    new_top_skills = ['algorithms', 'analytical', 'analytical skills', 'analytics', 'artificial intelligence', 'aws',\n",
    "                      'azure', 'beautiful soup', 'big data', 'business intelligence', 'c++', 'cloud', 'coding',\n",
    "                      'communication', 'computer science', 'computer vision', 'css', 'data analysis', 'data analyst',\n",
    "                      'data analytics', 'data collection', 'data management', 'data mining', 'data modeling',\n",
    "                      'data quality', 'data science', 'data scientist', 'data structures', 'data visualization',\n",
    "                      'deep learning', 'docker', 'excel', 'financial services', 'flask', 'forecasting', 'git', 'hadoop',\n",
    "                      'html', 'java', 'javascript', 'keras', 'logistic regression', 'machine learning', 'management',\n",
    "                      'matplotlib', 'natural language processing', 'neural networks', 'nlp', 'numpy', 'pandas', 'power bi',\n",
    "                      'predictive modeling', 'programming', 'project management', 'python', 'pytorch', 'r', 'react', 'sas',\n",
    "                      'scikit', 'scipy', 'seaborn', 'selenium', 'spark', 'sql', 'statistical modeling', 'statistics',\n",
    "                      'tableau', 'tensorflow', 'testing', 'web scraping']\n",
    "\n",
    "    count = 0\n",
    "    skills = []\n",
    "\n",
    "    for skill in cleaned_skills:\n",
    "        skill = skill.lower()\n",
    "        if skill in new_top_skills:\n",
    "            skills.append(skill)\n",
    "            count += 1\n",
    "\n",
    "    if count > 20:\n",
    "        final_score = 9\n",
    "    elif 15 <= count < 20:\n",
    "        final_score = 8\n",
    "    elif 10 <= count <= 14:\n",
    "        final_score = 7\n",
    "    elif 6 <= count <= 9:\n",
    "        final_score = 6\n",
    "    elif count < 2:\n",
    "        final_score = 1\n",
    "    else:\n",
    "        final_score = 4\n",
    "\n",
    "    remaining_skills = [i for i in new_top_skills if i not in skills]\n",
    "\n",
    "    return final_score, skills, remaining_skills\n",
    "\n",
    "def main():\n",
    "    st.title(\"Resume Score and Skills Checker\")\n",
    "\n",
    "    uploaded_file = st.file_uploader(\"Upload your resume (PDF)\", type=\"pdf\")\n",
    "\n",
    "    job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        # Read the uploaded PDF file\n",
    "        with uploaded_file:\n",
    "            resume_text = \"\"\n",
    "            pdf_reader = PdfReader(uploaded_file)\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                resume_text += pdf_reader.pages[page_num].extract_text()\n",
    "\n",
    "        # Fetch the score and skills\n",
    "        score, mentioned_skills, not_mentioned_skills = fetch_my_score(resume_text)\n",
    "\n",
    "        # Display the results\n",
    "        st.header(\"Resume Score and Skills\")\n",
    "        st.write(f\"*Score:* {score}\")\n",
    "        st.write(\"*Skills Mentioned in Resume:*\")\n",
    "        st.write(mentioned_skills)\n",
    "        st.write(\"*Skills Not Mentioned in Resume (Recommendations):*\")\n",
    "        st.write(not_mentioned_skills)\n",
    "\n",
    "        # Calculate Similarity\n",
    "        resume_vector = text_to_vector(remove_un(resume_text))\n",
    "        job_vector = text_to_vector(remove_un(job_description))\n",
    "        similarity = calculate_cosine_similarity(resume_vector, job_vector)\n",
    "\n",
    "        st.header(\"Similarity with Job Description\")\n",
    "        st.write(f\"Similarity: {similarity}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.title(\"Resume Score and Skills Checker\")\n",
    "\n",
    "    uploaded_file = st.file_uploader(\"Upload your resume (PDF)\", type=\"pdf\")\n",
    "\n",
    "    job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        # Read the uploaded PDF file\n",
    "        with uploaded_file:\n",
    "            resume_text = \"\"\n",
    "            pdf_reader = PdfReader(uploaded_file)\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                resume_text += pdf_reader.pages[page_num].extract_text()\n",
    "\n",
    "        # Fetch the score and skills\n",
    "        score, mentioned_skills, not_mentioned_skills = fetch_my_score(resume_text)\n",
    "\n",
    "        # Display the results\n",
    "        st.header(\"Resume Score and Skills\")\n",
    "        st.write(f\"*Score:* {score}\")\n",
    "        st.write(\"*Skills Mentioned in Resume:*\")\n",
    "        st.write(mentioned_skills)\n",
    "        st.write(\"*Skills Not Mentioned in Resume (Recommendations):*\")\n",
    "        st.write(not_mentioned_skills)\n",
    "\n",
    "        # Calculate Similarity\n",
    "        resume_vector = text_to_vector(remove_un(resume_text))\n",
    "        job_vector = text_to_vector(remove_un(job_description))\n",
    "        similarity = calculate_cosine_similarity(resume_vector, job_vector)\n",
    "\n",
    "        st.header(\"Similarity with Job Description\")\n",
    "        st.write(f\"Similarity: {similarity}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf591a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "          \n",
    "\n",
    "def find_all_text(resume_paths):\n",
    "    # Load the spaCy English model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def extract_info_from_resume(pdf_path):\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "            text = ''\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                text += pdf_reader.pages[page_num].extract_text()\n",
    "\n",
    "        # Process the text with spaCy\n",
    "        doc = nlp(text)\n",
    "\n",
    "\n",
    "        # Extract Education\n",
    "        education = extract_education(doc)\n",
    "\n",
    "        # Extract Skills\n",
    "        skills = extract_skills(text)\n",
    "\n",
    "        # Extract Projects\n",
    "        projects = extract_projects(doc)\n",
    "\n",
    "        # Extract Experience using specific keywords\n",
    "        experience = extract_experience_sentences(doc)\n",
    "\n",
    "        # Concatenate information into the 'All_Text' column\n",
    "        all_text = f\"Education: {education}\\n Skills: {skills}\\n Projects: {projects}\\n Experience: {experience}\"\n",
    "\n",
    "        #print(f\"All_Text: {all_text}\")\n",
    "\n",
    "        return {\n",
    "            'Education': education,\n",
    "            'Skills': skills,\n",
    "            'Projects': projects,\n",
    "            'Experience': experience,\n",
    "            'All_Text': all_text\n",
    "        }\n",
    "\n",
    "\n",
    "    def extract_education(doc):\n",
    "        # Initialize an empty list to store education information\n",
    "        education_info = []\n",
    "\n",
    "        # Define education-related keywords and labels\n",
    "        education_keywords = ['education', 'university', 'college', 'Masters', 'Bachelors', 'degree', 'institute', \n",
    "                              'institutes', 'school', 'graduation', 'postgraduate', 'undergraduate', 'diploma', \n",
    "                              'academy', 'IIT', 'BIT', 'NIT', 'VIT', 'VIIT', 'COEP', 'SIT', 'MIT', 'SIBM',\n",
    "                              'AISSMS', 'PICT', 'PCCOE', 'SPPU', 'DY PATIL', 'DIAT', 'AIT', 'MMCOE', 'SLS',\n",
    "                              'SYMBIOSIS', 'FC', 'AFMS', 'MILIT', 'SINHGAD', 'MCASC', 'SID', 'BMCC', 'SCAC', \n",
    "                              'IMED', 'SIIB', 'SIDTM', 'IIMS', 'FLAME', 'PUMBA', 'DYPIMS', 'NIA', 'NIBM', 'SSE', \n",
    "                              'IIIT', 'SCMIRT', 'IBMR', '%']\n",
    "\n",
    "        education_labels = ['EDU', 'ORG', 'PRODUCT', 'UNI', 'INST', 'SCH', 'DEG', 'DIP', 'CERT', 'ACAD', 'UNIV', 'COLL', 'SCHOOL']\n",
    "\n",
    "        # Iterate through entities to find education-related information\n",
    "        for ent in doc.ents:\n",
    "            # Check if the entity is related to education\n",
    "            if any(keyword in ent.text.lower() for keyword in education_keywords) and ent.label_ in education_labels:\n",
    "                # Extract the surrounding text to get the full education information\n",
    "                start = max(0, ent.start - 1)  # Adjust the window size as needed\n",
    "                end = min(len(doc), ent.end + 25)\n",
    "                education_info.append(doc[start:end].text.strip())\n",
    "\n",
    "        return ', '.join(education_info) if education_info else 'N/A'\n",
    "\n",
    "\n",
    "\n",
    "    def extract_skills(text):\n",
    "        skills_start = text.find('Skills')\n",
    "        skills_end = text.find('SKILLS') if text.find('SKILLS') != -1 else len(text)\n",
    "        skills_text = text[skills_start:skills_end].strip()\n",
    "\n",
    "        # Process the skills text with spaCy\n",
    "        doc = nlp(skills_text)\n",
    "\n",
    "        # Extract lines or sentences containing skill-related information\n",
    "        skill_sentences = [sent.text.strip() for sent in doc.sents if any(token.pos_ in ['NOUN', 'VERB', 'ADJ'] for token in sent)]\n",
    "\n",
    "        # Combine the extracted lines into a single string\n",
    "        skills = ' '.join(skill_sentences)\n",
    "\n",
    "        return skills if skills else 'N/A'\n",
    "\n",
    "    def extract_projects(doc):\n",
    "        # Initialize an empty list to store project information\n",
    "        projects = []\n",
    "\n",
    "        # Define project-related keywords and labels\n",
    "        project_keywords = ['project', 'developed', 'created', 'implemented', 'built']\n",
    "        project_labels = ['PROJECT', 'ORG', 'PRODUCT']\n",
    "\n",
    "        # Iterate through sentences to find project-related information\n",
    "        for ent in doc.ents:\n",
    "            # Check if the entity is related to projects\n",
    "            if any(keyword in ent.text.lower() for keyword in project_keywords) and ent.label_ in project_labels:\n",
    "                # Extract the surrounding text to get the full project information\n",
    "                start = max(0, ent.start - 1)  # Adjust the window size as needed\n",
    "                end = min(len(doc), ent.end + 135)\n",
    "                project_info = doc[start:end].text.strip()\n",
    "                projects.append(project_info)\n",
    "\n",
    "        return ', '.join(projects) if projects else 'N/A'\n",
    "\n",
    "\n",
    "    def extract_experience_sentences(doc):\n",
    "        # Find the start and end indices of the \"Experience\" section\n",
    "        start_index = doc.text.lower().find('experience')\n",
    "        end_index = doc.text.lower().find('skills') if doc.text.lower().find('skills') != -1 else len(doc.text)\n",
    "\n",
    "        # Extract sentences within the specified section\n",
    "        experience_section = doc.text[start_index:end_index]\n",
    "\n",
    "        # Process the experience section with spaCy\n",
    "        experience_doc = nlp(experience_section)\n",
    "\n",
    "        # Extract sentences containing keywords related to experience within the section\n",
    "        experience_keywords = ['work', 'experience', 'position', 'job']\n",
    "        experience_sentences = [sent.text.strip() for sent in experience_doc.sents if any(keyword in sent.text.lower() for keyword in experience_keywords)]\n",
    "\n",
    "        # Combine the extracted sentences into a single string\n",
    "        experience = ' '.join(experience_sentences)\n",
    "\n",
    "        return experience if experience else 'N/A'\n",
    "\n",
    "\n",
    "    def clean_and_tokenize_skills(skill_string):\n",
    "        if isinstance(skill_string, str):\n",
    "            # Remove '/', '\\n', extra spaces\n",
    "            cleaned_string = re.sub(r'[/\\\\]+|\\n|\\s+', ' ', skill_string)\n",
    "\n",
    "            # Tokenize the skills using nltk's word_tokenize\n",
    "            tokenized_skills = word_tokenize(cleaned_string)\n",
    "\n",
    "            return tokenized_skills\n",
    "        else:\n",
    "            # Return an empty list if the input is not a string\n",
    "            return []\n",
    "\n",
    "\n",
    "    all_resume_info = []\n",
    "\n",
    "    for resume_path in resume_paths:\n",
    "        resume_info = extract_info_from_resume(resume_path)\n",
    "        all_resume_info.append(resume_info)\n",
    "    all_text=[]\n",
    "    #df_candidates = pd.DataFrame(all_resume_info, columns=['Name', 'Email', 'Education', 'Skills', 'Projects', 'Experience','All_Text'])\n",
    "    for k in range(0,len(all_resume_info)):\n",
    "        for i,j in all_resume_info[k].items():\n",
    "            if i==\"All_Text\":\n",
    "                all_text.append(j)\n",
    "            \n",
    "    return all_text\n",
    "\n",
    "def remove_un(text):\n",
    "    if type(text) == str:\n",
    "        string = []\n",
    "        for i in text.split():\n",
    "            word = (\"\".join(e for e in i if e.isalnum()))\n",
    "            word = word.lower()\n",
    "\n",
    "            if word not in stopwords:\n",
    "                string.append(word)\n",
    "\n",
    "        return \" \".join(string)\n",
    "    elif type(text) == list:\n",
    "        result = []\n",
    "        for t in text:\n",
    "            if type(t) == str:\n",
    "                string = []\n",
    "                for i in t.split():\n",
    "                    word = (\"\".join(e for e in i if e.isalnum()))\n",
    "                    word = word.lower()\n",
    "\n",
    "                    if word not in stopwords:\n",
    "                        string.append(word)\n",
    "                result.append(\" \".join(string))\n",
    "\n",
    "        return result\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_name_from_pdf(pdf_path):\n",
    "    # Extract the base name (file name) from the file path\n",
    "    file_name = os.path.basename(pdf_path)\n",
    "    \n",
    "    # Remove the file extension (.pdf)\n",
    "    name_without_extension = os.path.splitext(file_name)[0]\n",
    "    \n",
    "    # Split the name using underscores or spaces\n",
    "    name_parts = re.split(r'[_\\s]+', name_without_extension)\n",
    "    \n",
    "    # Capitalize each part of the name\n",
    "    capitalized_name = ' '.join(part.capitalize() for part in name_parts)\n",
    "    \n",
    "    return capitalized_name    \n",
    "\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "# Function to convert text to vector\n",
    "def text_to_vector(text):\n",
    "    words = preprocess_text(text)\n",
    "    # Get word vectors for each word\n",
    "    word_vectors = [token.vector for token in nlp(\" \".join(words))]\n",
    "    # Average the word vectors to get the document vector\n",
    "    if word_vectors:\n",
    "        return sum(word_vectors) / len(word_vectors)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to convert job description to vector\n",
    "def job_to_vector(job_description):\n",
    "    return text_to_vector(job_description)\n",
    "\n",
    "# Function to calculate cosine similarity between two vectors\n",
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    if vector1 is not None and vector2 is not None:\n",
    "        return cosine_similarity([vector1], [vector2])[0][0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e020586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    words = preprocess_text(text)\n",
    "    # Get word vectors for each word\n",
    "    word_vectors = [token.vector for token in nlp(\" \".join(words))]\n",
    "    # Average the word vectors to get the document vector\n",
    "    if word_vectors:\n",
    "        return sum(word_vectors) / len(word_vectors)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def remove_un(text):\n",
    "    if type(text) == str:\n",
    "        string = []\n",
    "        for i in text.split():\n",
    "            word = (\"\".join(e for e in i if e.isalnum()))\n",
    "            word = word.lower()\n",
    "\n",
    "            if word not in stopwords:\n",
    "                string.append(word)\n",
    "\n",
    "        return \" \".join(string)\n",
    "    elif type(text) == list:\n",
    "        result = []\n",
    "        for t in text:\n",
    "            if type(t) == str:\n",
    "                string = []\n",
    "                for i in t.split():\n",
    "                    word = (\"\".join(e for e in i if e.isalnum()))\n",
    "                    word = word.lower()\n",
    "\n",
    "                    if word not in stopwords:\n",
    "                        string.append(word)\n",
    "                result.append(\" \".join(string))\n",
    "\n",
    "        return result\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    if vector1 is not None and vector2 is not None:\n",
    "        return cosine_similarity([vector1], [vector2])[0][0]\n",
    "    else:\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
